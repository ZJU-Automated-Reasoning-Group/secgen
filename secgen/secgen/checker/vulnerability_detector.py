"""Integrated vulnerability detector combining multiple analysis techniques."""

import asyncio
from pathlib import Path
from typing import Dict, List, Optional, Any, Set
from dataclasses import dataclass

from secgen.core.analyzer import CodeAnalyzer, Vulnerability, VulnerabilityType, Severity, CodeLocation, PathStep, VulnerabilityPath
from secgen.core.interprocedural_analyzer import InterproceduralAnalyzer
from secgen.checker.unified_taint_analyzer import UnifiedTaintAnalyzer
from secgen.checker.unified_memory_analyzer import UnifiedMemoryAnalyzer
from secgen.core.function_summarizer import FunctionSummarizer, LLMFunctionSummary


@dataclass
class AnalysisReport:
    """Comprehensive analysis report."""
    files_analyzed: List[str]
    total_vulnerabilities: int
    vulnerabilities_by_severity: Dict[str, int]
    vulnerabilities_by_type: Dict[str, int]
    vulnerabilities: List[Vulnerability]
    call_graph_metrics: Dict[str, Any]
    taint_analysis_summary: Dict[str, Any]
    memory_statistics: Dict[str, Any]
    function_summaries: Dict[str, LLMFunctionSummary]
    analysis_time: float
    confidence_distribution: Dict[str, int]


class VulnerabilityDetector:
    """Integrated vulnerability detector using multiple analysis techniques."""
    
    def __init__(self, model=None, logger=None, config: Optional[Dict] = None):
        """Initialize vulnerability detector.
        
        Args:
            model: LLM model for intelligent analysis
            logger: Logger instance
            config: Configuration options
        """
        self.model = model
        self.logger = logger
        self.config = config or {}
        
        # Initialize analyzers with proper dependencies
        self.code_analyzer = CodeAnalyzer(model, logger)
        self.interprocedural_analyzer = InterproceduralAnalyzer(model, logger)
        
        # Initialize unified analyzers with interprocedural context
        self.taint_analyzer = UnifiedTaintAnalyzer(model, logger, self.interprocedural_analyzer)
        self.memory_analyzer = UnifiedMemoryAnalyzer(model, logger, self.interprocedural_analyzer)
        self.function_summarizer = FunctionSummarizer(model, logger)
        
        # Analysis results
        self.all_vulnerabilities: List[Vulnerability] = []
        self.file_contents: Dict[str, str] = {}
        
    def analyze_project(self, project_path: str, 
                       file_extensions: Optional[List[str]] = None,
                       exclude_patterns: Optional[List[str]] = None) -> AnalysisReport:
        """Analyze entire project for vulnerabilities.
        
        Args:
            project_path: Path to project root
            file_extensions: File extensions to analyze
            exclude_patterns: Patterns to exclude from analysis
            
        Returns:
            Comprehensive analysis report
        """
        import time
        start_time = time.time()
        
        if self.logger:
            self.logger.log(f"Starting vulnerability analysis of {project_path}")
        
        # Set default extensions if not provided
        if file_extensions is None:
            file_extensions = ['.py', '.c', '.cpp', '.h', '.hpp', '.java', '.js', '.ts']
        
        if exclude_patterns is None:
            exclude_patterns = ['test', 'tests', '__pycache__', '.git', 'node_modules']
        
        # Find and read all relevant files
        files_to_analyze = self._find_files(project_path, file_extensions, exclude_patterns)
        
        if self.logger:
            self.logger.log(f"Found {len(files_to_analyze)} files to analyze")
        
        # Read file contents
        for file_path in files_to_analyze:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    self.file_contents[file_path] = f.read()
            except Exception as e:
                if self.logger:
                    self.logger.log(f"Error reading {file_path}: {e}", level="ERROR")
        
        # Perform different types of analysis
        all_vulnerabilities = []
        
        # 1. Basic static analysis
        if self.logger:
            self.logger.log("Performing static analysis...")
        static_results = self.code_analyzer.analyze_directory(project_path, file_extensions)
        all_vulnerabilities.extend(static_results.get('vulnerabilities', []))
        
        # 2. Interprocedural analysis
        if self.logger:
            self.logger.log("Performing interprocedural analysis...")
        if static_results.get('functions'):
            # Build call graph
            self.interprocedural_analyzer.build_call_graph(static_results['functions'])
            
            # Build function summaries for IDFS-style analysis
            function_summaries = self.interprocedural_analyzer.build_function_summaries(
                static_results['functions'], self.file_contents
            )
            
            # Perform interprocedural taint analysis using summaries
            interprocedural_taint_paths = self.interprocedural_analyzer.analyze_interprocedural_taint_flow()
            
            # Convert taint paths to vulnerabilities
            interprocedural_vulns = []
            for taint_path in interprocedural_taint_paths:
                # Create vulnerability path from taint path
                vuln_path = self._create_vulnerability_path_from_taint_path(taint_path)
                
                vuln = Vulnerability(
                    vuln_type=taint_path.vulnerability_type,
                    severity=Severity.HIGH,
                    location=CodeLocation(
                        file_path=taint_path.sink.function,
                        line_start=taint_path.sink.line_number,
                        line_end=taint_path.sink.line_number
                    ),
                    description=f"Interprocedural taint flow from {taint_path.source.function} to {taint_path.sink.function}",
                    evidence=f"Call path length: {len(taint_path.path)}",
                    confidence=taint_path.confidence,
                    recommendation="Validate and sanitize data flow between functions",
                    path=vuln_path
                )
                interprocedural_vulns.append(vuln)
            
            all_vulnerabilities.extend(interprocedural_vulns)
            
            # Also run traditional interprocedural analysis
            traditional_vulns = self.interprocedural_analyzer.detect_interprocedural_vulnerabilities(
                self.file_contents
            )
            all_vulnerabilities.extend(traditional_vulns)
        
        # 3. Enhanced taint analysis with interprocedural context
        if self.logger:
            self.logger.log("Performing enhanced taint analysis with interprocedural context...")
        
        if static_results.get('functions') and function_summaries:
            # Use interprocedural context for enhanced analysis
            taint_vulns = self.taint_analyzer.analyze_with_interprocedural_context(
                self.file_contents, static_results['functions'], function_summaries
            )
            all_vulnerabilities.extend(taint_vulns)
        else:
            # Fallback to traditional file-by-file analysis
            for file_path, content in self.file_contents.items():
                taint_vulns = self.taint_analyzer.analyze_file(file_path, content)
                all_vulnerabilities.extend(taint_vulns)
        
        # 4. Enhanced memory safety analysis with interprocedural context
        if self.logger:
            self.logger.log("Performing enhanced memory safety analysis with interprocedural context...")
        
        if static_results.get('functions') and function_summaries:
            # Use interprocedural context for enhanced analysis
            memory_vulns = self.memory_analyzer.analyze_with_interprocedural_context(
                self.file_contents, static_results['functions'], function_summaries
            )
            all_vulnerabilities.extend(memory_vulns)
        else:
            # Fallback to traditional file-by-file analysis
            for file_path, content in self.file_contents.items():
                memory_vulns = self.memory_analyzer.analyze_file(file_path, content)
                all_vulnerabilities.extend(memory_vulns)
        
        # 5. Function summarization
        if self.logger:
            self.logger.log("Generating function summaries...")
        function_summaries = {}
        if static_results.get('functions'):
            function_summaries = self.function_summarizer.summarize_functions_batch(
                static_results['functions'], 
                self.file_contents
            )
        
        # 6. Deduplicate and rank vulnerabilities
        unique_vulnerabilities = self._deduplicate_vulnerabilities(all_vulnerabilities)
        ranked_vulnerabilities = self._rank_vulnerabilities(unique_vulnerabilities)
        
        # 7. Generate comprehensive report
        analysis_time = time.time() - start_time
        
        report = self._generate_report(
            files_to_analyze,
            ranked_vulnerabilities,
            static_results,
            function_summaries,
            analysis_time
        )
        
        if self.logger:
            self.logger.log(f"Analysis completed in {analysis_time:.2f} seconds")
            self.logger.log(f"Found {len(ranked_vulnerabilities)} unique vulnerabilities")
        
        return report
    
    def _find_files(self, project_path: str, extensions: List[str], 
                   exclude_patterns: List[str]) -> List[str]:
        """Find all files to analyze in the project."""
        files = []
        project_root = Path(project_path)
        
        for ext in extensions:
            pattern = f"**/*{ext}"
            for file_path in project_root.glob(pattern):
                file_str = str(file_path)
                
                # Check exclude patterns
                should_exclude = False
                for exclude_pattern in exclude_patterns:
                    if exclude_pattern in file_str:
                        should_exclude = True
                        break
                
                if not should_exclude:
                    files.append(file_str)
        
        return files
    
    def _deduplicate_vulnerabilities(self, vulnerabilities: List[Vulnerability]) -> List[Vulnerability]:
        """Remove duplicate vulnerabilities."""
        seen = set()
        unique_vulns = []
        
        for vuln in vulnerabilities:
            # Create a signature for the vulnerability
            signature = (
                str(vuln.location),
                vuln.vuln_type.value,
                vuln.description
            )
            
            if signature not in seen:
                seen.add(signature)
                unique_vulns.append(vuln)
        
        return unique_vulns
    
    def _rank_vulnerabilities(self, vulnerabilities: List[Vulnerability]) -> List[Vulnerability]:
        """Rank vulnerabilities by severity and confidence."""
        
        # Define severity weights
        severity_weights = {
            Severity.CRITICAL: 5,
            Severity.HIGH: 4,
            Severity.MEDIUM: 3,
            Severity.LOW: 2,
            Severity.INFO: 1
        }
        
        def vulnerability_score(vuln):
            severity_score = severity_weights.get(vuln.severity, 1)
            confidence_score = vuln.confidence
            return severity_score * confidence_score
        
        # Sort by score (descending)
        ranked = sorted(vulnerabilities, key=vulnerability_score, reverse=True)
        
        return ranked
    
    def _generate_report(self, files_analyzed: List[str], 
                        vulnerabilities: List[Vulnerability],
                        static_results: Dict[str, Any],
                        function_summaries: Dict[str, LLMFunctionSummary],
                        analysis_time: float) -> AnalysisReport:
        """Generate comprehensive analysis report."""
        
        # Count vulnerabilities by severity
        severity_counts = {}
        for vuln in vulnerabilities:
            severity = vuln.severity.value
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
        
        # Count vulnerabilities by type
        type_counts = {}
        for vuln in vulnerabilities:
            vuln_type = vuln.vuln_type.value
            type_counts[vuln_type] = type_counts.get(vuln_type, 0) + 1
        
        # Confidence distribution
        confidence_ranges = {
            '0.9-1.0': 0,
            '0.8-0.9': 0,
            '0.7-0.8': 0,
            '0.6-0.7': 0,
            '0.0-0.6': 0
        }
        
        for vuln in vulnerabilities:
            conf = vuln.confidence
            if conf >= 0.9:
                confidence_ranges['0.9-1.0'] += 1
            elif conf >= 0.8:
                confidence_ranges['0.8-0.9'] += 1
            elif conf >= 0.7:
                confidence_ranges['0.7-0.8'] += 1
            elif conf >= 0.6:
                confidence_ranges['0.6-0.7'] += 1
            else:
                confidence_ranges['0.0-0.6'] += 1
        
        # Get metrics from individual analyzers
        call_graph_metrics = self.interprocedural_analyzer.get_call_graph_metrics()
        taint_summary = self.taint_analyzer.get_taint_summary()
        memory_stats = self.memory_analyzer.get_memory_statistics()
        
        return AnalysisReport(
            files_analyzed=files_analyzed,
            total_vulnerabilities=len(vulnerabilities),
            vulnerabilities_by_severity=severity_counts,
            vulnerabilities_by_type=type_counts,
            vulnerabilities=vulnerabilities,
            call_graph_metrics=call_graph_metrics,
            taint_analysis_summary=taint_summary,
            memory_statistics=memory_stats,
            function_summaries=function_summaries,
            analysis_time=analysis_time,
            confidence_distribution=confidence_ranges
        )
    
    async def enhance_with_llm_analysis(self, vulnerabilities: List[Vulnerability]) -> List[Vulnerability]:
        """Enhance vulnerability analysis using LLM."""
        if not self.model:
            return vulnerabilities
        
        if self.logger:
            self.logger.log("Enhancing analysis with LLM...")
        
        enhanced_vulnerabilities = []
        
        # Process vulnerabilities in batches to avoid overwhelming the LLM
        batch_size = 10
        for i in range(0, len(vulnerabilities), batch_size):
            batch = vulnerabilities[i:i + batch_size]
            
            # Enhance each vulnerability in the batch
            enhanced_batch = await self.taint_analyzer.enhance_with_llm(batch)
            enhanced_vulnerabilities.extend(enhanced_batch)
            
            if self.logger:
                self.logger.log(f"Enhanced {len(enhanced_batch)} vulnerabilities")
        
        return enhanced_vulnerabilities
    
    def generate_summary_report(self, report: AnalysisReport) -> str:
        """Generate a human-readable summary report."""
        
        summary = f"""
# Security Analysis Report

## Overview
- **Files Analyzed**: {len(report.files_analyzed)}
- **Total Vulnerabilities**: {report.total_vulnerabilities}
- **Analysis Time**: {report.analysis_time:.2f} seconds

## Vulnerability Breakdown by Severity
"""
        
        for severity, count in sorted(report.vulnerabilities_by_severity.items()):
            summary += f"- **{severity.upper()}**: {count}\n"
        
        summary += "\n## Vulnerability Types\n"
        for vuln_type, count in sorted(report.vulnerabilities_by_type.items()):
            summary += f"- **{vuln_type.replace('_', ' ').title()}**: {count}\n"
        
        summary += "\n## Confidence Distribution\n"
        for conf_range, count in report.confidence_distribution.items():
            summary += f"- **{conf_range}**: {count}\n"
        
        if report.call_graph_metrics:
            summary += f"""
## Call Graph Analysis
- **Functions**: {report.call_graph_metrics.get('num_functions', 0)}
- **Function Calls**: {report.call_graph_metrics.get('num_calls', 0)}
- **Max Call Depth**: {report.call_graph_metrics.get('max_depth', 0)}
"""
        
        if report.taint_analysis_summary:
            summary += f"""
## Taint Analysis
- **Taint Sources**: {report.taint_analysis_summary.get('total_sources', 0)}
- **Taint Sinks**: {report.taint_analysis_summary.get('total_sinks', 0)}
- **Exploitable Flows**: {report.taint_analysis_summary.get('exploitable_flows', 0)}
"""
        
        if report.memory_statistics:
            summary += f"""
## Memory Analysis
- **Memory Allocations**: {report.memory_statistics.get('total_allocations', 0)}
- **Potential Leaks**: {report.memory_statistics.get('potential_leaks', 0)}
"""
        
        if report.function_summaries:
            summary += f"""
## Function Analysis
- **Functions Analyzed**: {len(report.function_summaries)}
- **Security Hotspots**: {len([s for s in report.function_summaries.values() if s.security_concerns])}
- **Complex Functions**: {len([s for s in report.function_summaries.values() if s.complexity_score >= 4])}
"""
        
        # Add top vulnerabilities
        if report.vulnerabilities:
            summary += "\n## Top Vulnerabilities\n"
            for i, vuln in enumerate(report.vulnerabilities[:10], 1):
                summary += f"""
### {i}. {vuln.vuln_type.value.replace('_', ' ').title()}
- **Location**: {vuln.location}
- **Severity**: {vuln.severity.value.upper()}
- **Confidence**: {vuln.confidence:.1%}
- **Description**: {vuln.description}
- **Recommendation**: {vuln.recommendation or 'No specific recommendation provided'}
"""
        
        return summary
    
    def export_json_report(self, report: AnalysisReport) -> Dict[str, Any]:
        """Export report as JSON-serializable dictionary."""
        
        vulnerabilities_json = []
        for vuln in report.vulnerabilities:
            vulnerabilities_json.append(vuln.to_dict())
        
        return {
            'summary': {
                'files_analyzed': len(report.files_analyzed),
                'total_vulnerabilities': report.total_vulnerabilities,
                'analysis_time': report.analysis_time,
                'vulnerabilities_by_severity': report.vulnerabilities_by_severity,
                'vulnerabilities_by_type': report.vulnerabilities_by_type,
                'confidence_distribution': report.confidence_distribution
            },
            'vulnerabilities': vulnerabilities_json,
            'analysis_metrics': {
                'call_graph': report.call_graph_metrics,
                'taint_analysis': report.taint_analysis_summary,
                'memory_analysis': report.memory_statistics
            },
            'files_analyzed': report.files_analyzed
        }
    
    def filter_vulnerabilities(self, vulnerabilities: List[Vulnerability], 
                             min_severity: Severity = Severity.LOW,
                             min_confidence: float = 0.0,
                             vuln_types: Optional[Set[VulnerabilityType]] = None) -> List[Vulnerability]:
        """Filter vulnerabilities based on criteria."""
        
        severity_order = [Severity.INFO, Severity.LOW, Severity.MEDIUM, Severity.HIGH, Severity.CRITICAL]
        min_severity_idx = severity_order.index(min_severity)
        
        filtered = []
        for vuln in vulnerabilities:
            # Check severity
            if severity_order.index(vuln.severity) < min_severity_idx:
                continue
            
            # Check confidence
            if vuln.confidence < min_confidence:
                continue
            
            # Check vulnerability type
            if vuln_types and vuln.vuln_type not in vuln_types:
                continue
            
            filtered.append(vuln)
        
        return filtered
    
    def _create_vulnerability_path_from_taint_path(self, taint_path) -> VulnerabilityPath:
        """Create VulnerabilityPath from TaintPath."""
        # Create source step
        source_step = PathStep(
            location=CodeLocation(
                file_path=taint_path.source.function,
                line_start=taint_path.source.line_number,
                line_end=taint_path.source.line_number
            ),
            description=f"Taint source in function {taint_path.source.function}",
            node_type='source',
            variable=taint_path.source.variable,
            function_name=taint_path.source.function
        )
        
        # Create sink step
        sink_step = PathStep(
            location=CodeLocation(
                file_path=taint_path.sink.function,
                line_start=taint_path.sink.line_number,
                line_end=taint_path.sink.line_number
            ),
            description=f"Taint sink in function {taint_path.sink.function}",
            node_type='sink',
            variable=taint_path.sink.variable,
            function_name=taint_path.sink.function
        )
        
        # Create intermediate steps from path
        intermediate_steps = []
        for step in taint_path.path:
            if hasattr(step, 'function') and hasattr(step, 'line_number'):
                path_step = PathStep(
                    location=CodeLocation(
                        file_path=step.function,  # Simplified - would need actual file path
                        line_start=step.line_number,
                        line_end=step.line_number
                    ),
                    description=f"Data flow through function {step.function}",
                    node_type='propagation',
                    variable=step.variable if hasattr(step, 'variable') else None,
                    function_name=step.function
                )
                intermediate_steps.append(path_step)
        
        return VulnerabilityPath(
            source=source_step,
            sink=sink_step,
            intermediate_steps=intermediate_steps,
            sanitizers=[]  # Would need to extract from taint_path if available
        )
    