"""Integrated vulnerability detector combining multiple analysis techniques."""

import time
from collections import Counter
from pathlib import Path
from typing import Dict, List, Optional, Any, Set
from dataclasses import dataclass

from secgen.ir.file_analyzer import FileAnalyzer
from secgen.core.models import Vulnerability, VulnerabilityType, Severity, CodeLocation, PathStep, VulnerabilityPath
from secgen.core.interprocedural_analyzer import InterproceduralAnalyzer
from secgen.checker.c_taint_checker import CTaintChecker
from secgen.checker.python_taint_checker import PythonTaintChecker
from secgen.checker.c_memory_checker import CMemoryChecker
from secgen.core.function_summarizer import FunctionSummarizer, LLMFunctionSummary


@dataclass
class AnalysisReport:
    """Comprehensive analysis report."""
    files_analyzed: List[str]
    total_vulnerabilities: int
    vulnerabilities_by_severity: Dict[str, int]
    vulnerabilities_by_type: Dict[str, int]
    vulnerabilities: List[Vulnerability]
    call_graph_metrics: Dict[str, Any]
    taint_analysis_summary: Dict[str, Any]
    memory_statistics: Dict[str, Any]
    function_summaries: Dict[str, LLMFunctionSummary]
    analysis_time: float
    confidence_distribution: Dict[str, int]


class VulnerabilityDetector:
    """Integrated vulnerability detector using multiple analysis techniques."""
    
    def __init__(self, model=None, logger=None, config: Optional[Dict] = None):
        """Initialize vulnerability detector.
        
        Args:
            model: LLM model for intelligent analysis
            logger: Logger instance
            config: Configuration options
        """
        self.model = model
        self.logger = logger
        self.config = config or {}
        
        # Initialize analyzers with proper dependencies
        self.file_analyzer = FileAnalyzer(logger)
        self.interprocedural_analyzer = InterproceduralAnalyzer(model, logger)
        
        # Initialize analyzers with interprocedural context
        self.taint_analyzer = CTaintChecker(model, logger, self.interprocedural_analyzer)
        self.python_taint_analyzer = PythonTaintChecker(model, logger, self.interprocedural_analyzer)
        self.memory_analyzer = CMemoryChecker(model, logger, self.interprocedural_analyzer)
        self.function_summarizer = FunctionSummarizer(model, logger)
        
        # Analysis results
        self.all_vulnerabilities: List[Vulnerability] = []
        self.file_contents: Dict[str, str] = {}
        
    def analyze_project(self, project_path: str, 
                       file_extensions: Optional[List[str]] = None,
                       exclude_patterns: Optional[List[str]] = None) -> AnalysisReport:
        """Analyze entire project for vulnerabilities."""
        start_time = time.time()
        
        # Set defaults
        file_extensions = file_extensions or ['.py', '.c', '.cpp', '.h', '.hpp', '.java', '.js', '.ts']
        exclude_patterns = exclude_patterns or ['__pycache__', '.git', 'node_modules']
        
        # Find and read files
        files_to_analyze = self._find_files(project_path, file_extensions, exclude_patterns)
        self._load_file_contents(files_to_analyze)
        
        # Perform analysis
        static_results = self.file_analyzer.analyze_directory(project_path, file_extensions)
        all_vulnerabilities = static_results.get('vulnerabilities', [])
        
        # Interprocedural analysis
        function_summaries = {}
        if static_results.get('functions'):
            self.interprocedural_analyzer.build_call_graph(static_results['functions'])
            function_summaries = self.interprocedural_analyzer.build_function_summaries(
                static_results['functions'], self.file_contents
            )
            
            # Taint and memory analysis with context
            if function_summaries:
                all_vulnerabilities.extend(self.taint_analyzer.analyze_with_interprocedural_context(
                    self.file_contents, static_results['functions'], function_summaries
                ))
                all_vulnerabilities.extend(self.python_taint_analyzer.analyze_with_interprocedural_context(
                    self.file_contents, static_results['functions'], function_summaries
                ))
                all_vulnerabilities.extend(self.memory_analyzer.analyze_with_interprocedural_context(
                    self.file_contents, static_results['functions'], function_summaries
                ))
            else:
                # Fallback to file-by-file analysis
                for file_path, content in self.file_contents.items():
                    all_vulnerabilities.extend(self.taint_analyzer.analyze_file(file_path, content))
                    all_vulnerabilities.extend(self.python_taint_analyzer.analyze_file(file_path, content))
                    all_vulnerabilities.extend(self.memory_analyzer.analyze_file(file_path, content))
        
        # Generate function summaries
        if static_results.get('functions'):
            function_summaries = self.function_summarizer.summarize_functions_batch(
                static_results['functions'], self.file_contents
            )
        
        # Process results
        unique_vulnerabilities = self._deduplicate_vulnerabilities(all_vulnerabilities)
        ranked_vulnerabilities = self._rank_vulnerabilities(unique_vulnerabilities)
        
        return self._generate_report(
            files_to_analyze, ranked_vulnerabilities, static_results, 
            function_summaries, time.time() - start_time
        )
    
    def _find_files(self, project_path: str, extensions: List[str], 
                   exclude_patterns: List[str]) -> List[str]:
        """Find all files to analyze in the project."""
        files = []
        project_root = Path(project_path)
        
        for ext in extensions:
            pattern = f"**/*{ext}"
            for file_path in project_root.glob(pattern):
                file_str = str(file_path)
                
                # Check exclude patterns
                should_exclude = False
                for exclude_pattern in exclude_patterns:
                    if exclude_pattern in file_str:
                        should_exclude = True
                        break
                
                if not should_exclude:
                    files.append(file_str)
        
        return files
    
    def _load_file_contents(self, files_to_analyze: List[str]) -> None:
        """Load contents of all files to analyze."""
        for file_path in files_to_analyze:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    self.file_contents[file_path] = f.read()
            except Exception as e:
                if self.logger:
                    self.logger.log(f"Error reading {file_path}: {e}", level="ERROR")
    
    def _deduplicate_vulnerabilities(self, vulnerabilities: List[Vulnerability]) -> List[Vulnerability]:
        """Remove duplicate vulnerabilities."""
        seen = set()
        unique_vulns = []
        
        for vuln in vulnerabilities:
            signature = (str(vuln.location), vuln.vuln_type.value, vuln.description)
            if signature not in seen:
                seen.add(signature)
                unique_vulns.append(vuln)
        
        return unique_vulns
    
    def _rank_vulnerabilities(self, vulnerabilities: List[Vulnerability]) -> List[Vulnerability]:
        """Rank vulnerabilities by severity and confidence."""
        severity_weights = {Severity.CRITICAL: 5, Severity.HIGH: 4, Severity.MEDIUM: 3, 
                          Severity.LOW: 2, Severity.INFO: 1}
        
        def score(vuln):
            return severity_weights.get(vuln.severity, 1) * vuln.confidence
        
        return sorted(vulnerabilities, key=score, reverse=True)
    
    def _generate_report(self, files_analyzed: List[str], 
                        vulnerabilities: List[Vulnerability],
                        static_results: Dict[str, Any],
                        function_summaries: Dict[str, LLMFunctionSummary],
                        analysis_time: float) -> AnalysisReport:
        """Generate comprehensive analysis report."""
        
        # Count vulnerabilities by severity and type
        severity_counts = Counter(vuln.severity.value for vuln in vulnerabilities)
        type_counts = Counter(vuln.vuln_type.value for vuln in vulnerabilities)
        
        # Confidence distribution
        confidence_ranges = {'0.9-1.0': 0, '0.8-0.9': 0, '0.7-0.8': 0, '0.6-0.7': 0, '0.0-0.6': 0}
        for vuln in vulnerabilities:
            conf = vuln.confidence
            if conf >= 0.9: confidence_ranges['0.9-1.0'] += 1
            elif conf >= 0.8: confidence_ranges['0.8-0.9'] += 1
            elif conf >= 0.7: confidence_ranges['0.7-0.8'] += 1
            elif conf >= 0.6: confidence_ranges['0.6-0.7'] += 1
            else: confidence_ranges['0.0-0.6'] += 1
        
        return AnalysisReport(
            files_analyzed=files_analyzed,
            total_vulnerabilities=len(vulnerabilities),
            vulnerabilities_by_severity=dict(severity_counts),
            vulnerabilities_by_type=dict(type_counts),
            vulnerabilities=vulnerabilities,
            call_graph_metrics=self.interprocedural_analyzer.get_call_graph_metrics(),
            taint_analysis_summary={},  # TODO: Implement taint summary
            memory_statistics={},  # TODO: Implement memory statistics
            function_summaries=function_summaries,
            analysis_time=analysis_time,
            confidence_distribution=confidence_ranges
        )
    
    async def enhance_with_llm_analysis(self, vulnerabilities: List[Vulnerability]) -> List[Vulnerability]:
        """Enhance vulnerability analysis using LLM."""
        if not self.model:
            return vulnerabilities
        
        enhanced_vulnerabilities = []
        batch_size = 10
        for i in range(0, len(vulnerabilities), batch_size):
            batch = vulnerabilities[i:i + batch_size]
            # TODO: Implement LLM enhancement for vulnerabilities
            enhanced_vulnerabilities.extend(batch)
        
        return enhanced_vulnerabilities
    
    def generate_summary_report(self, report: AnalysisReport) -> str:
        """Generate a human-readable summary report."""
        summary = f"""# Security Analysis Report

## Overview
- **Files Analyzed**: {len(report.files_analyzed)}
- **Total Vulnerabilities**: {report.total_vulnerabilities}
- **Analysis Time**: {report.analysis_time:.2f} seconds

## Vulnerability Breakdown by Severity
"""
        
        for severity, count in sorted(report.vulnerabilities_by_severity.items()):
            summary += f"- **{severity.upper()}**: {count}\n"
        
        summary += "\n## Vulnerability Types\n"
        for vuln_type, count in sorted(report.vulnerabilities_by_type.items()):
            summary += f"- **{vuln_type.replace('_', ' ').title()}**: {count}\n"
        
        summary += "\n## Confidence Distribution\n"
        for conf_range, count in report.confidence_distribution.items():
            summary += f"- **{conf_range}**: {count}\n"
        
        # Add top vulnerabilities
        if report.vulnerabilities:
            summary += "\n## Top Vulnerabilities\n"
            for i, vuln in enumerate(report.vulnerabilities[:10], 1):
                summary += f"""
### {i}. {vuln.vuln_type.value.replace('_', ' ').title()}
- **Location**: {vuln.location}
- **Severity**: {vuln.severity.value.upper()}
- **Confidence**: {vuln.confidence:.1%}
- **Description**: {vuln.description}
- **Recommendation**: {vuln.recommendation or 'No specific recommendation provided'}
"""
        
        return summary
    
    def export_json_report(self, report: AnalysisReport) -> Dict[str, Any]:
        """Export report as JSON-serializable dictionary."""
        return {
            'summary': {
                'files_analyzed': len(report.files_analyzed),
                'total_vulnerabilities': report.total_vulnerabilities,
                'analysis_time': report.analysis_time,
                'vulnerabilities_by_severity': report.vulnerabilities_by_severity,
                'vulnerabilities_by_type': report.vulnerabilities_by_type,
                'confidence_distribution': report.confidence_distribution
            },
            'vulnerabilities': [vuln.to_dict() for vuln in report.vulnerabilities],
            'analysis_metrics': {
                'call_graph': report.call_graph_metrics.to_dict() if report.call_graph_metrics else {},
                'taint_analysis': report.taint_analysis_summary,
                'memory_analysis': report.memory_statistics
            },
            'files_analyzed': report.files_analyzed
        }
    
    def filter_vulnerabilities(self, vulnerabilities: List[Vulnerability], 
                             min_severity: Severity = Severity.LOW,
                             min_confidence: float = 0.0,
                             vuln_types: Optional[Set[VulnerabilityType]] = None) -> List[Vulnerability]:
        """Filter vulnerabilities based on criteria."""
        severity_order = [Severity.INFO, Severity.LOW, Severity.MEDIUM, Severity.HIGH, Severity.CRITICAL]
        min_severity_idx = severity_order.index(min_severity)
        
        return [vuln for vuln in vulnerabilities 
                if severity_order.index(vuln.severity) >= min_severity_idx
                and vuln.confidence >= min_confidence
                and (not vuln_types or vuln.vuln_type in vuln_types)]
    
    def _create_vulnerability_path_from_taint_path(self, taint_path) -> VulnerabilityPath:
        """Create VulnerabilityPath from TaintPath."""
        def create_step(location, desc, node_type, var=None):
            return PathStep(
                location=CodeLocation(
                    file_path=location.function,
                    line_start=location.line_number,
                    line_end=location.line_number
                ),
                description=desc,
                node_type=node_type,
                variable=var,
                function_name=location.function
            )
        
        source_step = create_step(taint_path.source, f"Taint source in function {taint_path.source.function}", 'source', taint_path.source.variable)
        sink_step = create_step(taint_path.sink, f"Taint sink in function {taint_path.sink.function}", 'sink', taint_path.sink.variable)
        
        intermediate_steps = []
        for step in taint_path.path:
            if hasattr(step, 'function') and hasattr(step, 'line_number'):
                intermediate_steps.append(create_step(step, f"Data flow through function {step.function}", 'propagation', getattr(step, 'variable', None)))
        
        return VulnerabilityPath(source=source_step, sink=sink_step, intermediate_steps=intermediate_steps, sanitizers=[])
    